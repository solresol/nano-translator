{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-sheep",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "import collections\n",
    "import scipy\n",
    "import nltk\n",
    "import sklearn.feature_extraction\n",
    "import sklearn.model_selection\n",
    "import sklearn.dummy\n",
    "import sklearn.tree\n",
    "import sklearn.ensemble\n",
    "import sklearn.linear_model\n",
    "import sklearn.svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reported-topic",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = pandas.read_excel(\"LibreOffice_Translations_sfx2messages_Tok Pisin.xlsx\", na_values='',\n",
    "                         dtype={'English': object, 'Tok Pisin': object})\n",
    "texts = texts[['English', 'Tok Pisin']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forbidden-knowing",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts.English = texts.English.map(str).str.replace('~','').str.replace('_','').str.upper()\n",
    "texts['Tok Pisin'] = texts['Tok Pisin'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infrared-founder",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts['english_source_length'] = texts.English.map(nltk.word_tokenize).map(len)\n",
    "texts.english_source_length.plot.hist(bins=38)\n",
    "texts['tokpisin_length'] = texts['Tok Pisin'].map(nltk.word_tokenize).map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-proposal",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_word_source_texts = texts[texts.english_source_length == 2]\n",
    "two_word_source_texts.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plastic-parish",
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_classifier():\n",
    "    return sklearn.ensemble.RandomForestClassifier(random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aboriginal-trading",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(df, already_translated_count, verbose=False, model_creator=default_classifier):\n",
    "    cvec = sklearn.feature_extraction.text.CountVectorizer()\n",
    "    all_sentences = []\n",
    "    correct_predictions = []\n",
    "    for english, tokpisin in zip(df.English, df['Tok Pisin']):\n",
    "        tokpisin_tokens = nltk.word_tokenize(tokpisin)\n",
    "        if len(tokpisin_tokens) < already_translated_count:\n",
    "            continue\n",
    "        left_sentence = []\n",
    "        for word in nltk.word_tokenize(english):\n",
    "            left_sentence.append(f\"english_{word}\")\n",
    "        for i, word in enumerate(tokpisin_tokens):\n",
    "            if i >= already_translated_count:\n",
    "                # If you have to have a consistent number of words\n",
    "                # return a null-word marker here\n",
    "                break\n",
    "            left_sentence.append(f\"tokpisin_{word}\")\n",
    "        sentence = \" \".join(left_sentence)\n",
    "        if already_translated_count >= len(tokpisin_tokens):\n",
    "            correct_prediction = \"[END]\"\n",
    "        else:\n",
    "            correct_prediction = tokpisin_tokens[already_translated_count]\n",
    "        if verbose:\n",
    "            print(sentence.replace(' ', ' + '),\"=\", correct_prediction, f\"|{english.lower()} -> {tokpisin.lower()}\")\n",
    "        all_sentences.append(sentence)\n",
    "        correct_predictions.append(correct_prediction)\n",
    "    if len(all_sentences) == 0:\n",
    "        return None, None\n",
    "    # This is where we make some sort of embedding of the sentence.\n",
    "    # If you have enough data, use word embeddings in a large R^n space and use deep learning.\n",
    "    # For low resource languages, use something denser.\n",
    "    X = cvec.fit_transform(all_sentences)\n",
    "    #model = sklearn.linear_model.LogisticRegression()\n",
    "    # Keeps outputing OL\n",
    "    if len(set(correct_predictions)) == 1:\n",
    "        model = sklearn.dummy.DummyClassifier(strategy='most_frequent')\n",
    "    else:\n",
    "        model = model_creator()\n",
    "    model.fit(X, correct_predictions)\n",
    "    return (cvec, model)\n",
    "\n",
    "def make_models(df, model_creator=default_classifier):\n",
    "    answer = {}\n",
    "    i = 0\n",
    "    while True:\n",
    "        answer[i] = make_model(df, i, model_creator=model_creator)\n",
    "        if answer[i] == (None, None):\n",
    "            del answer[i]\n",
    "            return answer\n",
    "        i += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "obvious-planet",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_model(two_word_source_texts, 12, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constant-history",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "translator = make_models(two_word_source_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-evidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslatorExhaustion(Exception):\n",
    "    pass\n",
    "\n",
    "def suggest_next_token(translator, english_sentence, tokens_output_so_far=None):\n",
    "    if tokens_output_so_far is None:\n",
    "        tokens_output_so_far = []\n",
    "    token_count = len(tokens_output_so_far)\n",
    "    if token_count not in translator:\n",
    "        # We know something went wrong, though\n",
    "        raise TranslatorExhaustion\n",
    "    cvec, model = translator[token_count]\n",
    "    left_sentence = []\n",
    "    for word in nltk.word_tokenize(english_sentence):\n",
    "        left_sentence.append(f\"english_{word}\")\n",
    "    left_sentence += [f\"tokpisin_{word}\" for word in tokens_output_so_far]\n",
    "    X = cvec.transform([\" \".join(left_sentence)])\n",
    "    predictions = model.predict(X)\n",
    "    return predictions[0]\n",
    "\n",
    "def suggest_translation(translator, english_sentence):\n",
    "    tokens = []\n",
    "    while True:\n",
    "        try:\n",
    "            suggestion = suggest_next_token(translator, english_sentence, tokens)\n",
    "        except TranslatorExhaustion:\n",
    "            return \" \".join(tokens) + \" [INCOMPLETE]\"\n",
    "        if suggestion == \"[END]\":\n",
    "            if len(tokens) == 0:\n",
    "                return \"\"\n",
    "            if tokens[-1] in \"?.!\":\n",
    "                return \" \".join(tokens[:-1]) + tokens[-1]\n",
    "            else:\n",
    "                return \" \".join(tokens)\n",
    "        tokens.append(suggestion)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organized-certification",
   "metadata": {},
   "outputs": [],
   "source": [
    "suggest_next_token(translator, \"LAST PAGE\", [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reported-dodge",
   "metadata": {},
   "outputs": [],
   "source": [
    "suggest_next_token(translator, \"LAST PAGE\", ['PEIJ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-despite",
   "metadata": {},
   "outputs": [],
   "source": [
    "suggest_next_token(translator, \"LAST PAGE\", ['PEIJ', 'ANTAP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attended-brazil",
   "metadata": {},
   "outputs": [],
   "source": [
    "suggest_translation(translator, \"LAST PAGE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statewide-jumping",
   "metadata": {},
   "outputs": [],
   "source": [
    "suggest_translation(translator, \"NEXT PAGE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vertical-forest",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_translator(df, model_creator=sklearn.ensemble.RandomForestClassifier):\n",
    "    loo = sklearn.model_selection.LeaveOneOut()\n",
    "    for train_index, test_index in loo.split(df):\n",
    "        train = df.iloc[train_index]\n",
    "        testing_sentence = df.iloc[test_index].iloc[0]\n",
    "        translator = make_models(train)\n",
    "        translated = suggest_translation(translator, testing_sentence.English)\n",
    "        if translated == testing_sentence['Tok Pisin']:\n",
    "            print(\"*****\",testing_sentence.English,\"translated correctly as\", translated)\n",
    "        else:\n",
    "            print(testing_sentence.English,\"=\", translated, \". Correct answer is\",testing_sentence['Tok Pisin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competitive-purchase",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_translator(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-flight",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
